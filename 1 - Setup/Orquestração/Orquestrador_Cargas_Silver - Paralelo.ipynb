{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae58b66b-c598-4040-9313-9ad8b1a8043a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Orquestrador de Cargas - Camada Silver - Vers√£o Paralelo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbe90eba-95aa-44d9-8d75-9576ce56aac8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Carregamento de Fun√ß√µes e Inicializa√ß√£o do Spark"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Databricks notebook source\n",
    "import json\n",
    "import uuid\n",
    "import time\n",
    "import traceback\n",
    "from datetime import datetime, date\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, LongType\n",
    "from pyspark.dbutils import DBUtils\n",
    "from pyspark.sql import Row\n",
    "\n",
    "\n",
    "# Inicializa contexto Spark e DBUtils\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "dbutils = DBUtils(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14b12cbb-cc6d-4c04-ba2b-fb216ce01b57",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Apontamento de json e tabela de logs"
    }
   },
   "outputs": [],
   "source": [
    "# Caminho do JSON de configura√ß√£o\n",
    "json_path = \"/Volumes/vitivinicultura/default/landing_zone/orquestrador_camada_silver_paralel.json\"\n",
    "\n",
    "# Nome completo da tabela no Unity Catalog\n",
    "tabela_logs = \"vitivinicultura.logs.pipeline_logs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9289038e-27dd-4ca5-8332-7058dec217c3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cria√ß√£o do schema para grava√ß√£o na tabela de logs"
    }
   },
   "outputs": [],
   "source": [
    "# ‚úÖ Define schema fixo (evita erro de infer√™ncia)\n",
    "schema_log = StructType([\n",
    "    StructField(\"log_id\", StringType(), True),\n",
    "    StructField(\"execution_id\", StringType(), True),\n",
    "    StructField(\"pipeline_name\", StringType(), True),\n",
    "    StructField(\"job_name\", StringType(), True),\n",
    "    StructField(\"status\", StringType(), True),\n",
    "    StructField(\"start_time\", TimestampType(), True),\n",
    "    StructField(\"end_time\", TimestampType(), True),\n",
    "    StructField(\"duration_sec\", LongType(), True),\n",
    "    StructField(\"data_execucao\", StringType(), True),\n",
    "    StructField(\"user\", StringType(), True),\n",
    "    StructField(\"environment\", StringType(), True),\n",
    "    StructField(\"error_message\", StringType(), True),\n",
    "    StructField(\"tipo_erro\", StringType(), True),\n",
    "    StructField(\"initial_row_count\", LongType(), True),\n",
    "    StructField(\"final_row_count\", LongType(), True),\n",
    "    StructField(\"rows_loaded\", LongType(), True),\n",
    "    StructField(\"target_table\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f27e506-3852-4dc4-97fe-eeb7bb0c0d23",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Fun√ß√£o para salvar o log"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def salvar_log(execution_id, pipeline_name, job_name, status, start_time, end_time, data_execucao, user, environment,\n",
    "               error_message=None, tipo_erro=None, rows_before=None, rows_after=None, rows_inserted=None, target_table=None):\n",
    "    duration = int((end_time - start_time).total_seconds())\n",
    "    log_id = str(uuid.uuid4())\n",
    "\n",
    "    log_data = [(log_id, execution_id, pipeline_name, job_name, status, start_time, end_time, duration,\n",
    "                 data_execucao, user, environment, error_message, tipo_erro, rows_before, rows_after, rows_inserted, target_table)]\n",
    "\n",
    "    df = spark.createDataFrame(log_data, schema=schema_log)\n",
    "    df.write.format(\"delta\").mode(\"append\").saveAsTable(tabela_logs)\n",
    "\n",
    "\n",
    "def get_user_safe():\n",
    "    try:\n",
    "        return dbutils.notebook.entry_point.getDbutils().notebook().getContext().userName().get()\n",
    "    except:\n",
    "        return \"unknown_user\"\n",
    "\n",
    "user = get_user_safe()\n",
    "environment = \"dev\"\n",
    "data_hoje = date.today().strftime(\"%Y-%m-%d\")\n",
    "execution_id = str(uuid.uuid4())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8a4122a-f746-4ce9-87cd-e62aea58521a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Leitura din√¢mica do JSON"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "with open(json_path, \"r\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "pipeline_name = config.get(\"pipeline_name\", \"pipeline_desconhecido\")\n",
    "fail_fast = config.get(\"fail_fast\", True)\n",
    "jobs = config[\"jobs\"]\n",
    "\n",
    "\n",
    "for job in jobs:\n",
    "    params = job.get(\"params\", {})\n",
    "    for k, v in params.items():\n",
    "        if isinstance(v, str) and \"{data_atual}\" in v:\n",
    "            params[k] = v.replace(\"{data_atual}\", data_hoje)\n",
    "    job[\"params\"] = params\n",
    "\n",
    "print(f\"üöÄ Pipeline iniciado: {pipeline_name} - {len(jobs)} jobs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e3f6779-4861-4415-a051-f7fc8ed6d5ef",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Estrutura√ß√£o das depend√™ncias entre as tabelas"
    }
   },
   "outputs": [],
   "source": [
    "jobs_by_name = {job[\"name\"]: job for job in jobs}\n",
    "dependencies = {job[\"name\"]: job.get(\"depends_on\", []) for job in jobs}\n",
    "completed, failed = set(), set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10f95aa8-828b-470b-ba8c-c17dd4418d34",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Execu√ß√£o do job para cada tabela"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def run_notebook(job):\n",
    "    job_name = job[\"name\"]\n",
    "    path = job[\"path\"]\n",
    "    params = job.get(\"params\", {})\n",
    "    target_table = job.get(\"target_table\", None)\n",
    "    start_time = datetime.now()\n",
    "    status = \"OK\"\n",
    "    error_message = None\n",
    "    rows_before = rows_after = rows_loaded = None\n",
    "\n",
    "    print(f\"\\n‚ñ∂Ô∏è Iniciando job: {job_name}\")\n",
    "    print(f\"üìÇ Notebook: {path}\")\n",
    "    print(f\"‚öôÔ∏è Params: {params}\")\n",
    "\n",
    "    # Contagem inicial da tabela destino\n",
    "    if target_table:\n",
    "        try:\n",
    "            rows_before = spark.table(target_table).count()\n",
    "        except:\n",
    "            rows_before = 0\n",
    "\n",
    "    try:\n",
    "        # Executa notebook\n",
    "        result = dbutils.notebook.run(path, timeout_seconds=3600, arguments=params)\n",
    "\n",
    "        # Contagem final\n",
    "        if target_table:\n",
    "            rows_after = spark.table(target_table).count()\n",
    "            rows_loaded = max(rows_after - (rows_before or 0), 0)\n",
    "\n",
    "        print(f\"‚úÖ Job conclu√≠do: {job_name} | Linhas carregadas: {rows_loaded}\")\n",
    "        end_time = datetime.now()\n",
    "\n",
    "    except Exception as e:\n",
    "        end_time = datetime.now()\n",
    "        status = \"ERROR\"\n",
    "        error_message = str(e)[:1000]\n",
    "        full_trace = traceback.format_exc()\n",
    "\n",
    "        # Classifica tipo de erro\n",
    "        if \"AnalysisException\" in full_trace:\n",
    "            tipo_erro = \"ERRO_SQL\"\n",
    "        elif \"Permission\" in full_trace:\n",
    "            tipo_erro = \"ERRO_PERMISSAO\"\n",
    "        elif \"java.io\" in full_trace:\n",
    "            tipo_erro = \"ERRO_IO\"\n",
    "        elif \"Delta\" in full_trace:\n",
    "            tipo_erro = \"ERRO_DELTA\"\n",
    "        else:\n",
    "            tipo_erro = \"ERRO_DESCONHECIDO\"\n",
    "\n",
    "         # Salva log imediato\n",
    "        log = salvar_log(\n",
    "            execution_id, pipeline_name, job_name, status,\n",
    "            start_time, end_time, data_hoje, user, environment,\n",
    "            error_message=error_message, tipo_erro=tipo_erro,\n",
    "            rows_before=rows_before, rows_after=rows_after,\n",
    "            rows_inserted=rows_loaded, target_table=target_table\n",
    "        )\n",
    "\n",
    "        if fail_fast:\n",
    "            raise e\n",
    "\n",
    "        # \n",
    "        return {\n",
    "            \"name\": job_name,\n",
    "            \"status\": status,\n",
    "            \"log\": log\n",
    "        }\n",
    "\n",
    "    # \n",
    "    log = salvar_log(\n",
    "        execution_id, pipeline_name, job_name, status,\n",
    "        start_time, end_time, data_hoje, user, environment,\n",
    "        error_message=None, tipo_erro=None,\n",
    "        rows_before=rows_before, rows_after=rows_after,\n",
    "        rows_inserted=rows_loaded, target_table=target_table\n",
    "    )\n",
    "\n",
    "    # √∫ltima linha da fun√ß√£o\n",
    "    return {\n",
    "        \"name\": job_name,\n",
    "        \"status\": status,\n",
    "        \"log\": log\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a06fb47-8e0a-4a42-93cf-7fc485709bc3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Execu√ß√£o do paralelismo"
    }
   },
   "outputs": [],
   "source": [
    "def execute_pipeline(max_parallel=5):\n",
    "    global completed, failed\n",
    "    all_jobs = set(jobs_by_name.keys())\n",
    "    futures = {}\n",
    "    results = []\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_parallel) as executor:\n",
    "        while completed.union(failed) != all_jobs:\n",
    "            # Descobre jobs prontos para execu√ß√£o (todas as depend√™ncias completas)\n",
    "            ready_jobs = [\n",
    "                j for j, deps in dependencies.items()\n",
    "                if j not in completed and j not in failed\n",
    "                and all(d in completed for d in deps)\n",
    "            ]\n",
    "\n",
    "          # Submete jobs prontos\n",
    "            for job_name in ready_jobs:\n",
    "                if job_name not in futures.values():\n",
    "                    futures[executor.submit(run_notebook, jobs_by_name[job_name])] = job_name\n",
    "\n",
    "            # Processa resultados conforme finalizam\n",
    "            for future in as_completed(list(futures.keys())):\n",
    "                job_name = futures[future]\n",
    "                result = future.result()\n",
    "                results.append(result)\n",
    "                del futures[future]\n",
    "\n",
    "                if result[\"status\"] == \"OK\":\n",
    "                    completed.add(job_name)\n",
    "                else:\n",
    "                    failed.add(job_name)\n",
    "                    if fail_fast:\n",
    "                        print(f\"üõë Interrompendo pipeline por erro (fail_fast).\")\n",
    "                        return results\n",
    "\n",
    "            time.sleep(1)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a29f6490-9de2-4673-b2b9-43929876674e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Execu√ß√£o principal e grava log"
    }
   },
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# üèÅ Execu√ß√£o principal\n",
    "# =========================================\n",
    "start_time = time.time()\n",
    "results = execute_pipeline(max_parallel=5)\n",
    "total_time = round(time.time() - start_time, 2)\n",
    "\n",
    "\n",
    "# üîπ Consolidar logs\n",
    "try:\n",
    "    # Filtra apenas execu√ß√µes com log v√°lido\n",
    "    all_logs = []\n",
    "    for r in results:\n",
    "        if r and \"log\" in r and isinstance(r[\"log\"], dict):\n",
    "            all_logs.append(r[\"log\"])\n",
    "\n",
    "    if all_logs:\n",
    "        # Cria DataFrame\n",
    "        logs_df = spark.createDataFrame([Row(**log) for log in all_logs], schema=schema_log)\n",
    "\n",
    "        # Grava no Delta (append)\n",
    "        (\n",
    "            logs_df.write\n",
    "            .format(\"delta\")\n",
    "            .mode(\"append\")\n",
    "            .option(\"mergeSchema\", \"true\")  # seguran√ßa para evolu√ß√µes de schema\n",
    "            .saveAsTable(tabela_logs)\n",
    "        )\n",
    "\n",
    "        print(f\"üßæ {len(all_logs)} logs gravados com sucesso em {tabela_logs}\")\n",
    "\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Nenhum log foi retornado para grava√ß√£o.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erro ao salvar logs: {str(e)}\")\n",
    "\n",
    "\n",
    "# =========================================\n",
    "# üìä Resumo final\n",
    "# =========================================\n",
    "success_count = sum(1 for r in results if r[\"status\"] == \"OK\")\n",
    "fail_count = sum(1 for r in results if r[\"status\"] == \"ERROR\")\n",
    "\n",
    "print(\"\\nüìã RESUMO FINAL\")\n",
    "print(f\"Pipeline: {pipeline_name}\")\n",
    "print(f\"Dura√ß√£o total: {total_time}s\")\n",
    "print(f\"Jobs conclu√≠dos: {success_count}\")\n",
    "print(f\"Jobs com erro: {fail_count}\")\n",
    "print(f\"‚úÖ Conclu√≠dos: {completed}\")\n",
    "if failed:\n",
    "    print(f\"‚ùå Falharam: {failed}\")\n",
    "else:\n",
    "    print(\"‚úÖ Todos os jobs executados com sucesso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0a473e7-8448-47bb-a67c-b8335741c620",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"job_name\":182,\"target_table\":363,\"pipeline_name\":130,\"error_message\":451},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1763408117198}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM vitivinicultura.logs.pipeline_logs\n",
    "\n",
    "--DELETE FROM vitivinicultura.logs.pipeline_logs\n",
    "\n",
    "--INSERT INTO vitivinicultura.logs.pipeline_logs FROM vitivinicultura.logs.pipeline_logs\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": "HIGH"
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1445578670306814,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Orquestrador_Cargas_Silver - Paralelo",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae58b66b-c598-4040-9313-9ad8b1a8043a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Orquestrador de Cargas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24dbeb9e-dc6f-4a5b-b4ef-b0e1bd257547",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "import json\n",
    "import uuid\n",
    "import traceback\n",
    "from datetime import datetime, date\n",
    "from pyspark.sql import Row, SparkSession\n",
    "from pyspark.dbutils import DBUtils\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, LongType\n",
    "\n",
    "# Inicializa contexto Spark e DBUtils\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "dbutils = DBUtils(spark)\n",
    "\n",
    "# Caminho do JSON de configura√ß√£o\n",
    "json_path = \"/Volumes/vitivinicultura/default/landing_zone/orquestrador_teste.json\"\n",
    "\n",
    "# Nome completo da tabela no Unity Catalog\n",
    "tabela_logs = \"vitivinicultura.logs.pipeline_logs\"\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# üîπ Fun√ß√£o auxiliar: salva log de execu√ß√£o no Unity Catalog\n",
    "# --------------------------------------------------------\n",
    "# ‚úÖ Define schema fixo (evita erro de infer√™ncia)\n",
    "schema_log = StructType([\n",
    "    StructField(\"log_id\", StringType(), True),\n",
    "    StructField(\"execution_id\", StringType(), True),\n",
    "    StructField(\"pipeline_name\", StringType(), True),\n",
    "    StructField(\"job_name\", StringType(), True),\n",
    "    StructField(\"status\", StringType(), True),\n",
    "    StructField(\"start_time\", TimestampType(), True),\n",
    "    StructField(\"end_time\", TimestampType(), True),\n",
    "    StructField(\"duration_sec\", LongType(), True),\n",
    "    StructField(\"data_execucao\", StringType(), True),\n",
    "    StructField(\"user\", StringType(), True),\n",
    "    StructField(\"environment\", StringType(), True),\n",
    "    StructField(\"error_message\", StringType(), True),\n",
    "    StructField(\"initial_row_count\", LongType(), True),\n",
    "    StructField(\"final_row_count\", LongType(), True),\n",
    "    StructField(\"rows_loaded\", LongType(), True)\n",
    "])\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# üîπ Fun√ß√£o auxiliar: salva log de execu√ß√£o no Unity Catalog\n",
    "# --------------------------------------------------------\n",
    "def salvar_log(execution_id, pipeline_name, job_name, status, start_time, end_time, data_execucao, user, environment,\n",
    "               rows_before=None, rows_after=None, rows_inserted=None, error_message=None):\n",
    "    duration = int((end_time - start_time).total_seconds())\n",
    "    log_id = str(uuid.uuid4())\n",
    "    log_data = [(log_id, execution_id, pipeline_name, job_name, status,\n",
    "                 start_time, end_time, duration, data_execucao,\n",
    "                 user, environment, rows_before, rows_after, rows_inserted, error_message)]\n",
    "    \n",
    "    df = spark.createDataFrame(log_data, schema=schema_log)\n",
    "    df.write.format(\"delta\").mode(\"append\").saveAsTable(tabela_logs)\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# üîπ Carrega configura√ß√£o JSON\n",
    "# --------------------------------------------------------\n",
    "with open(json_path, \"r\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "pipeline_name = config[\"pipeline_name\"]\n",
    "fail_fast = config.get(\"fail_fast\", True)\n",
    "data_hoje = date.today().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Substitui placeholders din√¢micos ({data_atual})\n",
    "for job in config[\"jobs\"]:\n",
    "    params = job.get(\"params\", {})\n",
    "    for k, v in params.items():\n",
    "        if isinstance(v, str) and \"{data_atual}\" in v:\n",
    "            params[k] = v.replace(\"{data_atual}\", data_hoje)\n",
    "    job[\"params\"] = params\n",
    "\n",
    "executados = set()\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# üîπ Metadados din√¢micos de execu√ß√£o\n",
    "# --------------------------------------------------------\n",
    "execution_id = str(uuid.uuid4())\n",
    "def get_user_safe():\n",
    "    try:\n",
    "        return dbutils.notebook.entry_point.getDbutils().notebook().getContext().userName().get()\n",
    "    except:\n",
    "        return \"unknown_user\"\n",
    "\n",
    "user = get_user_safe()\n",
    "\n",
    "\n",
    "environment = \"dev\"  # Altere para \"prod\" ou \"test\" conforme o cluster\n",
    "\n",
    "print(f\"üö¶ Iniciando pipeline: {pipeline_name}\")\n",
    "print(f\"üÜî Execution ID: {execution_id}\")\n",
    "print(f\"üë§ Usu√°rio: {user}\")\n",
    "print(f\"üåé Ambiente: {environment}\")\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# üîπ Fun√ß√£o para executar cada job\n",
    "# --------------------------------------------------------\n",
    "def run_job(job_name):\n",
    "    job = next(j for j in config[\"jobs\"] if j[\"name\"] == job_name)\n",
    "    deps = job.get(\"depends_on\", [])\n",
    "\n",
    "    for dep in deps:\n",
    "        if dep not in executados:\n",
    "            run_job(dep)\n",
    "\n",
    "    print(f\"üöÄ Executando {job['name']} com params {job['params']}\")\n",
    "    start_time = datetime.now()\n",
    "    status = \"OK\"\n",
    "    error_message = None\n",
    "    initial_row_count = None\n",
    "    final_row_count = None\n",
    "\n",
    "    # üîπ Detecta se o job referencia uma tabela de destino\n",
    "    tabela_destino = job.get(\"target_table\", None)\n",
    "\n",
    "    # Se existir tabela de destino, captura a contagem inicial\n",
    "    if tabela_destino:\n",
    "        try:\n",
    "            initial_row_count = spark.table(tabela_destino).count()\n",
    "        except:\n",
    "            initial_row_count = 0\n",
    "\n",
    "    try:\n",
    "        dbutils.notebook.run(job[\"path\"], 0, job[\"params\"])\n",
    "\n",
    "        # Se tiver tabela destino, conta ap√≥s a carga\n",
    "        if tabela_destino:\n",
    "            final_row_count = spark.table(tabela_destino).count()\n",
    "\n",
    "    except Exception as e:\n",
    "        status = \"ERROR\"\n",
    "        error_message = str(e)[:1000]\n",
    "        print(f\"‚ùå Falha em {job['name']}: {error_message}\")\n",
    "        if fail_fast:\n",
    "            end_time = datetime.now()\n",
    "            salvar_log(execution_id, pipeline_name, job[\"name\"], status, start_time, end_time,\n",
    "                       job[\"params\"][\"data_execucao\"], user, environment, error_message,\n",
    "                       initial_row_count, final_row_count)\n",
    "            raise e\n",
    "    finally:\n",
    "        end_time = datetime.now()\n",
    "        salvar_log(execution_id, pipeline_name, job[\"name\"], status, start_time, end_time,\n",
    "                   job[\"params\"][\"data_execucao\"], user, environment, error_message,\n",
    "                   initial_row_count, final_row_count)\n",
    "        executados.add(job_name)\n",
    "        print(f\"‚úÖ Finalizado {job['name']} com status {status}\")\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# üîπ Execu√ß√£o principal do pipeline\n",
    "# --------------------------------------------------------\n",
    "for job in config[\"jobs\"]:\n",
    "    if job[\"name\"] not in executados:\n",
    "        run_job(job[\"name\"])\n",
    "\n",
    "print(f\"üèÅ Pipeline {pipeline_name} finalizado com sucesso.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0a473e7-8448-47bb-a67c-b8335741c620",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"log_id\":270},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1762367882014}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM vitivinicultura.logs.pipeline_logs\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6824231582327434,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Orquestrador Cargas",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

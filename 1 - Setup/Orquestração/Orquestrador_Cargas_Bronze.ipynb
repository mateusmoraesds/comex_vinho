{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae58b66b-c598-4040-9313-9ad8b1a8043a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Orquestrador de Cargas - Camada Bronze\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24dbeb9e-dc6f-4a5b-b4ef-b0e1bd257547",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "import json\n",
    "import uuid\n",
    "import traceback\n",
    "from datetime import datetime, date\n",
    "from pyspark.sql import Row, SparkSession\n",
    "from pyspark.dbutils import DBUtils\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, LongType\n",
    "\n",
    "# Inicializa contexto Spark e DBUtils\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "dbutils = DBUtils(spark)\n",
    "\n",
    "# Caminho do JSON de configuraÃ§Ã£o\n",
    "json_path = \"/Volumes/vitivinicultura/default/landing_zone/orquestrador_camada_bronze.json\"\n",
    "\n",
    "# Nome completo da tabela no Unity Catalog\n",
    "tabela_logs = \"vitivinicultura.logs.pipeline_logs\"\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# ğŸ”¹ FunÃ§Ã£o auxiliar: salva log de execuÃ§Ã£o no Unity Catalog\n",
    "# --------------------------------------------------------\n",
    "# âœ… Define schema fixo (evita erro de inferÃªncia)\n",
    "schema_log = StructType([\n",
    "    StructField(\"log_id\", StringType(), True),\n",
    "    StructField(\"execution_id\", StringType(), True),\n",
    "    StructField(\"pipeline_name\", StringType(), True),\n",
    "    StructField(\"job_name\", StringType(), True),\n",
    "    StructField(\"status\", StringType(), True),\n",
    "    StructField(\"start_time\", TimestampType(), True),\n",
    "    StructField(\"end_time\", TimestampType(), True),\n",
    "    StructField(\"duration_sec\", LongType(), True),\n",
    "    StructField(\"data_execucao\", StringType(), True),\n",
    "    StructField(\"user\", StringType(), True),\n",
    "    StructField(\"environment\", StringType(), True),\n",
    "    StructField(\"error_message\", StringType(), True),\n",
    "    StructField(\"initial_row_count\", LongType(), True),\n",
    "    StructField(\"final_row_count\", LongType(), True),\n",
    "    StructField(\"rows_loaded\", LongType(), True)\n",
    "])\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# ğŸ”¹ FunÃ§Ã£o auxiliar: salva log de execuÃ§Ã£o no Unity Catalog\n",
    "# --------------------------------------------------------\n",
    "def salvar_log(execution_id, pipeline_name, job_name, status, start_time, end_time, data_execucao, user, environment,\n",
    "               rows_before=None, rows_after=None, rows_inserted=None, error_message=None):\n",
    "    duration = int((end_time - start_time).total_seconds())\n",
    "    log_id = str(uuid.uuid4())\n",
    "    log_data = [(log_id, execution_id, pipeline_name, job_name, status,\n",
    "                 start_time, end_time, duration, data_execucao,\n",
    "                 user, environment, rows_before, rows_after, rows_inserted, error_message)]\n",
    "    \n",
    "    df = spark.createDataFrame(log_data, schema=schema_log)\n",
    "    df.write.format(\"delta\").mode(\"append\").saveAsTable(tabela_logs)\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# ğŸ”¹ Carrega configuraÃ§Ã£o JSON\n",
    "# --------------------------------------------------------\n",
    "#with open(json_path, \"r\") as f:\n",
    "#    config = json.loads(f.read())\n",
    "json_str = dbutils.fs.head(\n",
    "    json_path,\n",
    "    10485760  # up to 10MB\n",
    ")\n",
    "config = json.loads(json_str)\n",
    "\n",
    "pipeline_name = config[\"pipeline_name\"]\n",
    "fail_fast = config.get(\"fail_fast\", True)\n",
    "data_hoje = date.today().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Substitui placeholders dinÃ¢micos ({data_atual})\n",
    "for job in config[\"jobs\"]:\n",
    "    params = job.get(\"params\", {})\n",
    "    for k, v in params.items():\n",
    "        if isinstance(v, str) and \"{data_atual}\" in v:\n",
    "            params[k] = v.replace(\"{data_atual}\", data_hoje)\n",
    "    job[\"params\"] = params\n",
    "\n",
    "executados = set()\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# ğŸ”¹ Metadados dinÃ¢micos de execuÃ§Ã£o\n",
    "# --------------------------------------------------------\n",
    "execution_id = str(uuid.uuid4())\n",
    "def get_user_safe():\n",
    "    try:\n",
    "        return dbutils.notebook.entry_point.getDbutils().notebook().getContext().userName().get()\n",
    "    except:\n",
    "        return \"unknown_user\"\n",
    "\n",
    "user = get_user_safe()\n",
    "\n",
    "\n",
    "environment = \"dev\"  # Altere para \"prod\" ou \"test\" conforme o cluster\n",
    "\n",
    "print(f\"ğŸš¦ Iniciando pipeline: {pipeline_name}\")\n",
    "print(f\"ğŸ†” Execution ID: {execution_id}\")\n",
    "print(f\"ğŸ‘¤ UsuÃ¡rio: {user}\")\n",
    "print(f\"ğŸŒ Ambiente: {environment}\")\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# ğŸ”¹ FunÃ§Ã£o para executar cada job\n",
    "# --------------------------------------------------------\n",
    "def run_job(job_name):\n",
    "    job = next(j for j in config[\"jobs\"] if j[\"name\"] == job_name)\n",
    "    deps = job.get(\"depends_on\", [])\n",
    "\n",
    "    for dep in deps:\n",
    "        if dep not in executados:\n",
    "            run_job(dep)\n",
    "\n",
    "    print(f\"ğŸš€ Executando {job['name']} com params {job['params']}\")\n",
    "    start_time = datetime.now()\n",
    "    status = \"OK\"\n",
    "    error_message = None\n",
    "    initial_row_count = None\n",
    "    final_row_count = None\n",
    "\n",
    "    # ğŸ”¹ Detecta se o job referencia uma tabela de destino\n",
    "    tabela_destino = job.get(\"target_table\", None)\n",
    "\n",
    "    # Se existir tabela de destino, captura a contagem inicial\n",
    "    if tabela_destino:\n",
    "        try:\n",
    "            initial_row_count = spark.table(tabela_destino).count()\n",
    "        except:\n",
    "            initial_row_count = 0\n",
    "\n",
    "    try:\n",
    "        print(f\"\\nğŸš€ Iniciando execuÃ§Ã£o do notebook:\")\n",
    "        print(f\"ğŸ“˜ Nome do job: {job['name']}\")\n",
    "        print(f\"ğŸ“‚ Caminho: {job['path']}\")\n",
    "        print(f\"âš™ï¸ ParÃ¢metros: {job['params']}\")\n",
    "\n",
    "        # Executa o notebook e captura retorno\n",
    "        result = dbutils.notebook.run(job[\"path\"], 0, job[\"params\"])\n",
    "        print(f\"ğŸŸ¢ Retorno do notebook {job['name']}: {result}\")\n",
    "\n",
    "        # Se tiver tabela destino, conta apÃ³s a carga\n",
    "        if tabela_destino:\n",
    "            final_row_count = spark.table(tabela_destino).count()\n",
    "\n",
    "    except Exception as e:\n",
    "        status = \"ERROR\"\n",
    "        end_time = datetime.now()\n",
    "        error_str = str(e)\n",
    "        error_type = type(e).__name__\n",
    "        error_message = error_str[:1000]  # limita tamanho\n",
    "\n",
    "        # ğŸ§  Identifica origem do erro\n",
    "        if \"NotebookExecutionException\" in error_str or \"WorkflowException\" in error_str:\n",
    "            origem = \"notebook filho\"\n",
    "        elif \"Py4JJavaError\" in error_str:\n",
    "            origem = \"spark engine (executor ou driver)\"\n",
    "        else:\n",
    "            origem = \"notebook pai\"\n",
    "\n",
    "        # ğŸ¯ ClassificaÃ§Ã£o de tipo de erro\n",
    "        if \"AnalysisException\" in error_str:\n",
    "            categoria = \"Erro SQL / tabela inexistente\"\n",
    "        elif \"Permission\" in error_str:\n",
    "            categoria = \"Erro de permissÃ£o\"\n",
    "        elif \"OutOfMemoryError\" in error_str:\n",
    "            categoria = \"Erro de memÃ³ria / cluster\"\n",
    "        elif \"Table\" in error_str and \"not found\" in error_str:\n",
    "            categoria = \"Tabela nÃ£o encontrada\"\n",
    "        elif \"TypeError\" in error_str or \"ValueError\" in error_str:\n",
    "            categoria = \"Erro de cÃ³digo Python\"\n",
    "        elif \"Network\" in error_str or \"Connection\" in error_str:\n",
    "            categoria = \"Erro de conexÃ£o / rede\"\n",
    "        else:\n",
    "            categoria = \"Erro genÃ©rico\"\n",
    "\n",
    "        # ğŸ§¾ Log estruturado e legÃ­vel\n",
    "        print(\"â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\")\n",
    "        print(\"âŒ ERRO DETECTADO DURANTE EXECUÃ‡ÃƒO DO NOTEBOOK\")\n",
    "        print(f\"ğŸ“˜ Job: {job['name']}\")\n",
    "        print(f\"ğŸ“‚ Caminho: {job['path']}\")\n",
    "        print(f\"ğŸ—ï¸ Origem: {origem}\")\n",
    "        print(f\"âš™ï¸ Categoria: {categoria}\")\n",
    "        print(f\"ğŸ’¥ Tipo: {error_type}\")\n",
    "        print(f\"ğŸ§¾ Mensagem: {error_message}\")\n",
    "        print(\"â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\")\n",
    "\n",
    "        # ğŸ’¾ Log detalhado no histÃ³rico\n",
    "        salvar_log(\n",
    "            execution_id,\n",
    "            pipeline_name,\n",
    "            job[\"name\"],\n",
    "            status,\n",
    "            start_time,\n",
    "            end_time,\n",
    "            job[\"params\"][\"data_execucao\"],\n",
    "            user,\n",
    "            environment,\n",
    "            f\"{origem} | {categoria} | {error_message}\",\n",
    "            initial_row_count,\n",
    "            final_row_count\n",
    "        )\n",
    "\n",
    "        # ğŸš¨ Fail fast opcional\n",
    "        if fail_fast:\n",
    "            raise e\n",
    "\n",
    "    finally:\n",
    "        end_time = datetime.now()\n",
    "        salvar_log(\n",
    "            execution_id,\n",
    "            pipeline_name,\n",
    "            job[\"name\"],\n",
    "            status,\n",
    "            start_time,\n",
    "            end_time,\n",
    "            job[\"params\"][\"data_execucao\"],\n",
    "            user,\n",
    "            environment,\n",
    "            error_message,\n",
    "            initial_row_count,\n",
    "            final_row_count\n",
    "        )\n",
    "        executados.add(job_name)\n",
    "        print(f\"âœ… Finalizado {job['name']} com status {status}\")\n",
    "\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# ğŸ”¹ ExecuÃ§Ã£o principal do pipeline\n",
    "# --------------------------------------------------------\n",
    "for job in config[\"jobs\"]:\n",
    "    if job[\"name\"] not in executados:\n",
    "        run_job(job[\"name\"])\n",
    "\n",
    "print(f\"ğŸ Pipeline {pipeline_name} finalizado com sucesso.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0a473e7-8448-47bb-a67c-b8335741c620",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"job_name\":215},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1762537967638}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM vitivinicultura.logs.pipeline_logs\n",
    "\n",
    "--DELETE FROM vitivinicultura.logs.pipeline_logs\n",
    "\n",
    "--INSERT INTO vitivinicultura.logs.pipeline_logs FROM vitivinicultura.logs.pipeline_logs\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": "HIGH"
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1914785980901596,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Orquestrador_Cargas_Bronze",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
